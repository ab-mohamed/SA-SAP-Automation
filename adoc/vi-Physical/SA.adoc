
== Physical

////
The physical elements are included as an extension to the Technology Layer for modeling the physical world. Could here be Networking, Landscape considerations

* *_Where_* the resulting solution may physically or virtually reside
////

The SAP automation consists of several building blocks.
First we want to look at the infrastructure deployment with terraform and then to the salt part.

[NOTE]
---
As the project is under active development to make it better and simpler to use, this document focuses on the project version 6.0.0 of the terraform part and v6 of the rpm packages for salt formulas.
New version could have more features or slightly changed files as shown here, but the general guidelines should
still be applicable.
---

=== First make sure that all *pre-requirements* are done:

ifeval::[ "{cloud}" == "Azure" ]

. Have an Azure account
. Have installed the Azure command line tool _az_
. Have installed _terraform_ (v12) (it comes with SLES within the public cloud module)
. Have the SAP HANA install media downloaded from SAP
. Have created an Azure File Share
. Copy or write down the the name of the storage account and the storage key, which is similar to a password.
. Copy the SAP HANA install media to the Azure fileshare
. extract the install media

endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS - S3 bucket
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
GCP - GCP storage
endif::[]

ifeval::[ "{cloud}" == "Libvirt" ]
Libvirt - NFS share
endif::[]

As suggestion, we recommend the following directory structure:
//fixme check directory structure
----
share
  ├─ HANA
  │    └─ ...
  └─ Netweaver
       ├─ sapkernel
       │    ├─ ...
       ├─ export
       │    ├─ ...
       ├─ client
       │    ├─ ...
       └─ swpm
            ├─ ...
----

=== Get the project from the github site

The project is hosted on a public github site where you can download it to your local machine.

After moving into this directory you will see the following directory structure:

----
├── aws
├── azure
├── doc
├── gcp
├── generic_modules
├── libvirt
├── LICENSE
├── pillar
├── pillar_examples
├── README.md
└── salt
----

The directories with the names of the _cloud provider_ (aws,azure,gcp,libvirt) are the terraform templates for the relevant provider.

The _doc_ directory has some short but important documents for certain parts of the solution

The directory _generic_modules_ provides modules which get used by all cloud vendor templates, like common variables, local executed functions within the building block, dependent actions on destroy and the functions to start the SALT execution on the module building blocks.

The other directories _pillar_, _pillar_examples_ and _salt_ contain the part of the salt configuration management.

=== Terraform Building Blocks

Terraform relies on so called "providers" to interact with remote systems.

Providers are plugins and released independent from Terraform itself, this mean that each provider has its own series of version numbers.

Each Terraform module must declare which providers it requires, so that Terraform can install and use them.

If we switch to into a _cloud provider_ directory, we see one directory _modules_ and several _.tf_ files which all together build the terraform template.

When creating Terraform configurations, it's best practice to separate out parts of the configuration into individual .tf files. This provides better organization and readability.
----
├── infrastructure.tf
├── main.tf
├── modules
├── outputs.tf
├── README.md
├── terraform.tfvars
├── terraform.tfvars.example
└── variables.tf
----

The _infrastructure.tf_ file:: provides the cloud specific setup with the relevant provider module of terraform and defines all the needed cloud specific entities.

The _main.tf_ file:: provides all the values for the various variables needed for the modules. It is the main entry point for terraform.

The _modules_ directory:: provides more subdirectories which are the nested child modules which represent our technical building blocks

The _output.tf_ file:: provides the values handed out from the modules, e.g to be used or displayed.

_terraform.tfvars__:: is a variable definitions file which get automatically loaded instead of providing values manually and is the main file and maybe the only terrafrom file which should get changed.

_terraform.tfvars.example_:: is a example file with many pre-filled key-value pairs to set up
the solution. *You can use this as starting point for your own file.*

_variables.tf_:: provides all input variables, including a short description, the type of the variable and a default value which can be overwritten with the terraform.tfvars file.
Please have a deep look at all variable and the comments for it, to get aware whats is possible.

e.g. the variable provisioner is like a switch to run either the salt or terraform portion only


A _module_:: is a container for multiple resources that are used together. Modules can be used to create lightweight abstractions, so that you can describe your infrastructure in terms of its architecture, rather than directly in terms of physical objects.
+
We use the modules as our technical building blocks e.g. a HANA node and the module directory consist again  _main.tf_, _variables.tf_, _outputs.tf_.
+
These are the recommended filenames for a minimal module, even if they're empty. _main.tf_ is the primary entrypoint how to build the infrastructure building block.

There is one additional file (salt_provisioner.tf), which take care of handing over the needed values to the salt building blocks, with help of a special terraform resource called _null_provider_, and _triggers_ the remote execution of salt after terraform has created the node with _connections_ to the new created machines.
It now starts the _provision_ of the salt part.
It also create user-defined variables for salt as grains file (/tmp/grains) for the respective module building block

=== Simple Install

We provide a example terraform and example pillar files to provide a very easy start.

. Open a browser and goto https://github.com/SUSE/ha-sap-terraform-deployments
. Click on _tags_
. Click on _6.0.0_
+
You see what's new and what has changed - so if you use older versions make sure you read it carefully.
+
The _Usage_ section provides you with a link to a OpenBuildServer (OBS) repository where the RPM packages of the above discussed building blocks are stored which fit to the project version.
+
You need to use this value within the terraform variables file. So copy the line as described

. Now go to _Assets_ and download the _Source code_ as .zip or .tar.gz
. Extract it into a folder on your computer
. Goto this folder and into the sub folder _azure_
. Copy the file _terraform.tfvars.example_ to _terraform.tfvars.example_
    You will see many key-value variable pairs, some enabled some disabled with a _=_ in front.
    To have a simple start, only touch what we describe below
. So you need to change the region where you want to deploy the solution, so change _az_region = "westeurope"_ to the azure region you want to use

. To make it more easy to start, please change all 4 images types to pay-as-you-go (PAYG) to do so replace all _offer_ settings with  "sles-sap-15-sp2" and _sku_ with 15
+
Do this for hana, iscsi, monitoring, drbd e.g.

    hana_public_offer     = "SLES-SAP-BYOS"
    hana_public_sku       = "12-sp4"
+
with

    hana_public_offer = "sles-sap-15-sp2"
    hana_public_sku   = "gen2"
+
This will make use of the on-demand images which have automatically all needed SUSE repositories attached.

. Next is to set the name of the _admin_user_ to a name which you want to use

. The next step is to provide ssh keys to access the machines which will be deployed.
+
We recommend to create new keys for this as you need to provide both keys as they need to get copied to the machines.
So change the two locations variables and point them to your files.

. As we need SAP Install Media for the automatic deployment of HANA, you need to create a azure storage account where you need to copy the HANA media. Best would be if you already have extracted the SAP media to save time during the deployment.
+
Then we need to provide the name,key and the path to this storage account to the system.
So change

    storage_account_name
    storage_account_key
    hana_inst_master
+
The inst_master variable should point to the directory where you have the extracted hana install files.
There are more possibilities, but for the simples usage have everything already extracted on your share
+
So disable the other hana variables with adding a '#' in front of them

   #hana_archive_file = "IMDB_SERVER.SAR"
   #hana_sapcar_exe = "SAPCAR"
   #hana_extract_dir = "/sapmedia/HDBSERVER"

. We need additional ssh keys for the cluster communications, so please save your changes and run the following commands from the azure directory
+
[subs="attributes,quotes"]
----
   mkdir -p ../salt/hana_node/files/sshkeys
   ssh-keygen -t rsa -N '' -f ../salt/hana_node/files/sshkeys/cluster.id_rsa
----

. Please open the tfvars file again as we need a few final changes.
+
To create a HANA Systemreplication HA automation uncomment

    #hana_ha_enabled = true
+
by removing the _#_
+
As now the system creates a cluster, we need to enable a few other services. Uncomment

    #hana_cluster_sbd_enabled = true
+
by removing the _#_

. Now we need to point the place where the right packages for the v6 could be found. Copy the variable from step 1 e.g.
+
[subs="attributes,quotes"]
----
    ha_sap_deployment_repo = "https://download.opensuse.org/repositories/network:ha-clustering:sap-deployments:v6"
----

. If you want the additional monitoring be deployed, simply uncomment

    #monitoring_enabled = true

. As last step we enable a simplification parameter which try to find out a few settings automatically. So scroll down to the end and uncomment

    #pre_deployment = true

Now we have all settings for terraform done and are nearly at the step to run the deployment, so save your changes.

. go one directory up and change into the _pillar_example_ directory and here into the _automatic_ directory where you can see 3 further directories. They will provide the configuration variable for the relevant services. This automatic folder will work for all cloud providers we support today, but is more complex as it normally need to be.

. As we use only HANA, please switch to the _hana_ directory and open the file _hana_sls_.
+
The file looks quite complex - but we only need to change a few settings. Normally you would do provide a more simple file with your dedicated settings, but as we want to do it automatic, we use this file.

. We need to change the PRIMARY_SITE_NAME with some name you want to set and also the name for the SECONDARY_SITE_NAME.
You can change other settings like the passwords, but for a simple test you can leave it.
So save you changes and go back to the main directory.

. Now we are ready to run terraform
[subs="attributes,quotes"]
----
    az login
    terraform init
    terraform workspace new yourprojectname
    terraform plan
    terraform apply
----

If all goes well after ~40 Minutes you will have a installed and running HANA System Replication Cluster in Azure

As there is a jumphost with a public ip adress is created you simply can login to all machines from your machine with

    ssh -J <adminuser>@jumphost <adminuser@targethost>

==== Terraform file details

All files in the Terraform directory using the .tf file format will be automatically loaded during operations.

The _infrastructure.tf_ provides the _data sources_ for the network setup, which are computed in other terraform parts and some _locals_ variables used for mainly for the autogeneration of the network. In addition it provides the _resources_ for the network setup with virtual network, subnets and routing, resourcegroup to be used, a storage account, the all the network security groups (nsg) being used and defines the jumphost.

The _main.tf_ file is our main file and calls the child modules which consist of our building blocks and the required input and output variables defined by the child module.
It in addition provides the calculation for the autogenerated ip addresses.

There is the (default) possibility to autogenerate network addresses for all nodes.
For that you need to remove or comment all the variables related to the ip addresses (more information in variables.tf). With this approach all the addresses will be retrieved based in the provided virtual network addresses range (vnet_address_range).

ifeval::[ "{cloud}" == "Azure" ]

.Autogenerated addresses example based on 10.74.0.0/16 vnet address range and 10.74.0.0/24 subnet address range
[with="70%",options="header"]
|==========================
| Name         | Terraform variable | IP Address | Comment
| iSCSI server | iscsi_srv_ip       | 10.74.0.4  | needed for SBD device in HA configuration
| Monitoring   | monitoring_srv_ip  | 10.74.0.5  | if monitoring is enabled
| HANA IP's    | hana_ips           | 10.74.0.10, 10.74.0.11 | second only used in HA
| Hana cluster virtual IP | hana_cluster_vip | 10.74.0.12 | Only used if HA is enabled in HANA
| Hana cluster virtual IP secondary | hana_cluster_vip_secondary | 10.74.0.13 | Only used if the Active/Active HA setup is enabled
| DRBD IP's    | drbd_ips | 10.74.0.20, 10.74.0.21 | needed if HA NFS service for NW is used
| DRBD cluster vIP | drbd_cluster_vip | 10.74.0.22 |needed if HA NFS service for NW is used
| Netweaver IP's | netweaver_ips | 10.74.0.30, 10.74.0.31, 10.74.0.32, 10.74.0.33 | Addresses for the ASCS, ERS, PAS and AAS. The sequence will continue if there are more AAS machines
| Netweaver virtual IP's | netweaver_virtual_ips | 10.74.0.34, 10.74.0.35, 10.74.0.36, 192.168.135.37 | The 1st virtual address will be the next in the sequence of the regular Netweaver addresses
|==========================

endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS

Within AWS, the Availability Zones (AZ) of a VPC get used for the HA scenario.
Each of the AZ's has it's own network and therefore each of the machines in a cluster is in a different subnet. The floating virtual IP address is created with help of a special resource agent which changes the routing table entry of a virtual router for VPC, so the adress is outside of the VPC and AZ's

Example based on `10.0.0.0/16` address range (VPC address range) and `192.168.1.0/24` as `virtual_address_range` (the default value):

[with="70%",options="header"]
|==========================
| Name | Substituted variable | Addresses | Comments |
| :---: | :---: | :----: | :---: |
| Iscsi server | `iscsi_srv_ip` | `10.0.0.4` ||
| Monitoring | `monitoring_srv_ip` | `10.0.0.5` ||
| Hana ips | `hana_ips` | `10.0.1.10`, `10.0.2.11` ||
| Hana cluster vip | `hana_cluster_vip` | `192.168.1.10` | Only used if HA is enabled in HANA |
| Hana cluster vip secondary | `hana_cluster_vip_secondary` | `192.168.1.11` | Only used if the Active/Active setup is used |
| DRBD ips | `drbd_ips` | `10.0.5.20`, `10.0.6.21` ||
| DRBD cluster vip | `drbd_cluster_vip` | `192.168.1.20` ||
| Netweaver ips | `netweaver_ips` | `10.0.3.30`, `10.0.4.31`, `10.0.3.32`, `10.0.4.33` | Addresses for the ASCS, ERS, PAS and AAS. The sequence will continue if there are more AAS machines |
| Netweaver virtual ips | `netweaver_virtual_ips` | `192.168.1.30`, `192.168.1.31`, `192.168.1.32`, `192.168.1.33` | The last number of the address will match with the regular address |
|==========================
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
GCP

Example based on `10.0.0.0/24` VPC address range. The virtual addresses must be outside of the VPC address range.

[with="70%",options="header"]
|==========================
| Name | Substituted variable | Addresses | Comments |
| :---: | :---: | :----: | :---: |
| Iscsi server | `iscsi_srv_ip` | `10.0.0.4` ||
| Monitoring | `monitoring_srv_ip` | `10.0.0.5` ||
| Hana ips | `hana_ips` | `10.0.0.10`, `10.0.0.11` ||
| Hana cluster vip | `hana_cluster_vip` | `10.0.2.12` | Only used if HA is enabled in HANA |
| Hana cluster vip secondary | `hana_cluster_vip_secondary` | `10.0.1.13` | Only used if the Active/Active setup is used |
| DRBD ips | `drbd_ips` | `10.0.0.20`, `10.0.0.21` ||
| DRBD cluster vip | `drbd_cluster_vip` | `10.0.1.22` ||
| Netweaver ips | `netweaver_ips` | `10.0.0.30`, `10.0.0.31`, `10.0.0.32`, `10.0.0.33` | Addresses for the ASCS, ERS, PAS and AAS. The sequence will continue if there are more AAS machines |
| Netweaver virtual ips | `netweaver_virtual_ips` | `10.0.1.34`, `10.0.1.35`, `10.0.1.36`, `10.0.1.37` | The 1st virtual address will be the next in the sequence of the regular Netweaver addresses |
|==========================
endif::[]

ifeval::[ "{cloud}" == "Libvirt" ]
Libvirt

Example based on `192.168.135.0/24` address range:

[with="70%",options="header"]
|==========================
| Name | Substituted variable | Addresses | Comments |
| :---: | :---: | :----: | :---: |
| Iscsi server | `iscsi_srv_ip` | `192.168.135.4` ||
| Monitoring | `monitoring_srv_ip` | `192.168.135.5` ||
| Hana ips | `hana_ips` | `192.168.135.10`, `192.168.135.11` ||
| Hana cluster vip | `hana_cluster_vip` | `192.168.135.12` | Only used if HA is enabled in HANA |
| Hana cluster vip secondary | `hana_cluster_vip_secondary` | `192.168.135.13` | Only used if the Active/Active setup is used |
| DRBD ips | `drbd_ips` | `192.168.135.20`, `192.168.135.21` ||
| DRBD cluster vip | `drbd_cluster_vip` | `192.168.135.22` ||
| Netweaver ips | `netweaver_ips` | `192.168.135.30`, `192.168.135.31`, `192.168.135.32`, `192.168.135.33` | Addresses for the ASCS, ERS, PAS and AAS. The sequence will continue if there are more AAS machines |
| Netweaver virtual ips | `netweaver_virtual_ips` | `192.168.135.34`, `192.168.135.35`, `192.168.135.36`, `192.168.135.37` | The 1st virtual address will be the next in the sequence of the regular Netweaver addresses |
|==========================
endif::[]

If you want to use already existing network resources (virtual network and subnets) it can be done by configuring the _terraform.tfvars_ file and adjusting the responsible variables.

The example of how to use them is available at _terraform.tfvars.example_.

[IMPORTANT]
====
If you are specifying the IP addresses manually, make sure these are valid IP addresses. They should not be currently in use by existing instances. In the case of shared account usage in cloud providers, it is recommended to set unique addresses with each deployment to avoid using same addresses.
====

The _output.tf_ file is a way to expose some of the internal attributes, and act like the return values of a terraform module to the user. It will return the IP address and node names created from the automation.

The values defined in the _variables.tf_ file are used to avoid hard-coding parameters and provides all needed terraform input variables and there default values within the solution instead of having them in the main.tf file.

As we have many variable values to input, so we define them in a variable definition file named _terraform.tfvars_ and terraform will automatically load the variable values from the variable definition file if it is named terraform.tfvars


The _modules_ directory provide all the needed resources to create the respective building block

----
modules/
├── bastion
│   ├── main.tf
│   ├── outputs.tf
│   ├── salt_provisioner.tf
│   └── variables.tf
├── drbd_node
│   ├── main.tf
│   ├── outputs.tf
│   ├── salt_provisioner.tf
│   └── variables.tf
├── hana_node
│   ├── main.tf
│   ├── outputs.tf
│   ├── salt_provisioner.tf
│   └── variables.tf
├── iscsi_server
│   ├── main.tf
│   ├── outputs.tf
│   ├── salt_provisioner.tf
│   └── variables.tf
├── monitoring
│   ├── main.tf
│   ├── outputs.tf
│   ├── salt_provisioner.tf
│   └── variables.tf
├── netweaver_node
│   ├── main.tf
│   ├── outputs.tf
│   ├── salt_provisioner.tf
│   └── variables.tf
└── os_image_reference
    ├── outputs.tf
    └── variables.tf
----

The respective file _salt_provisioner.tf_ set the *_role_* of the *node* and handover the needed variables which where set in terraform, *as custom Salt _grains_ for the node* with help of a terraform file provisioner and starts the salt provisioning process via  
// #TODO via ?


==== SAP Sizing

One of the very important points to consider of a SAP deployment is sizing and applies across three key areas: compute power, storage space and i/o capacity and network bandwith.

If this is a greenfield deployment, please use the SAP Quick Sizer tool to calculate the SAP Application Performance Standard (SAPS) compute requirement and choose the right instance types which have the closest match to the performance needed.

If you have an SAP system running that you want to extend with new functionality and/or add new users or migrate to SAP HANA perform brownfield sizing.

Overall it is an iterative and constant process to translate your business requirements to the right (virtual) hardware resources.

This is a mandatory step and should not be underestimated.


ifeval::[ "{cloud}" == "Azure" ]

If you have some performance number, we want to make it easier to deploy the right instance sizes with the right disks types and performance, and the right network settings, we introduced a simplified SAP sizing which well known T-Shirt sizes, S,M,L and a very small Demo size.

Behind the sizes, are useful combinations to provide certain SAP performance scenarios.
Below is a simple reference of the possible performance values

* Demo
* Small  <  30.000 SAPS
* Medium <  70.000 SAPS
* Large  < 180.000 SAPS

You can simply customize the used settings within the terraform.tfvars, or more permanent in the variables file.

The Demo and Small size are thought for non-production scenarios and do not use SAP certified instancetypes, whereas the Medium and Large are meant for production usage and therefor use SAP certified instance types. The setups also used the right disks and I/O behavior for production.

The SAPS values are meant for the landscape and not only for the database.


===== HANA

Given that low storage latency is critical for database systems, even for in-memory systems as SAP HANA. The critical path in storage is usually around the transaction log writes of the DB systems, but other operations like savepoints or loading data in-memory after crash recovery can be critical.

Therefore, it is mandatory to leverage Azure premium storage or Ultra disk for /hana/data and /hana/log volumes. Depending on the performance requirements, we may need to build a RAID-0 stripe-set to aggregate IOPS and throughput to meet the application scenario need.

The overall VM I/O throughput and IOPS limits need to kept in mind when deciding for a instance type.

Actual recommendations could be looked at the following link
https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/hana-vm-operations-storage

The maps below, describes how the disks for SAP HANA will be used and created during the provisioning.

disks_type:: as HANA has high I/O requirements the disk type Premium SSD need to be used
disks_size:: is the size of the additional disk in GB, as every size has certain IOPS caps
caching:: The caching recommendations for Azure premium disks are assuming the I/O
characteristics for SAP HANA
/hana/data - no caching or read caching
/hana/log - no caching - exception for M- and Mv2-Series VMs where Azure Write Accelerator should be enabled
/hana/shared - read caching

writeaccelerator:: Azure Write Accelerator is a functionality that is available for Azure M-Series VMs exclusively. As the name states, the purpose of the functionality is to improve I/O latency of writes against the Azure premium storage. For SAP HANA, Write Accelerator is supposed to be used against the /hana/log volume only. Therefore, the /hana/data and /hana/log are separate volumes with Azure Write Accelerator supporting the /hana/log volume only.

Number of Disks:: The number of disks which get used, depend on the performance requirements. We join disks to a stripe set to provide more performance. At a minimum we need 4 to 5 disks.

LogicalVolumes::  We are using LVM to build stripe sets across several Azure premium disks. These stripe sizes differ between /hana/data and /hana/log and the recommendations is
256 KB for /hana/data
64 KB for /hana/log

Name of the VolumeGroup:: The name of the volume group used

Mount path:: The mount point where the volume gets mounted

The number of elements *must match* in all of them

_#_ character:: is used to split the volume groups
_,_ (comma):: is used to define the logical volumes for each volume group

The number of groups splitted by "#" *must match* in all of the entries

_names_:: The names of the volume groups (example datalog#shared#usrsap#backup#sapmnt)

_luns_:: The luns or disks used for each volume group. The number of luns must match with the configured in the previous disks variables (example 0,1,2#3#4#5#6)

_sizes_:: The size dedicated for each logical volume and folder. Example 70,100#100#100#100#100

_paths_:: Folder where each volume group will be mounted. Example /hana/data,/hana/log#/hana/shared#/usr/sap#/hana/backup#/sapmnt/

The values could be set with the variables "hana_vm_size", "hana_enable_accelerated_networking" and "hana_data_disks_configuration" in the _variables.tf_ file if you want to change the default (demo) or better in the _terraform.tfvars_ to set actual values.

===== Netweaver

NetWeaver is SAP's integrated technology platform and is not a product in itself, but it provides the needed services for the SAP business applications and always need a database to talk to.

So it's the overall task of sizing need to take care of Netweaver plus the Database and this is what we combined with the T-Shirt sizes of the solution.


====== Demo
Here the detail for the demo size

HANA instance size:: Standard_E4s_v3 with xx vCPU and yy GB memory
Accelerated networking:: false

.HANA disk configuration details
----
  disks_type       = "Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS"
  disks_size       = "128,128,128,128,128,128,128"
  caching          = "None,None,None,None,None,None,None"
  writeaccelerator = "false,false,false,false,false,false,false"
  luns             = "0,1#2,3#4#5#6#7"
  names            = "data#log#shared#usrsap#backup"
  lv_sizes         = "100#100#100#100#100"
  paths            = "/hana/data#/hana/log#/hana/shared#/usr/sap#/hana/backup"
----

.Netweaver configuration variables
|==========================
|netweaver_xscs_vm_size = "Standard_D2s_v3"
|netweaver_app_vm_size = "Standard_D2s_v3"
|netweaver_data_disk_type = "Premium_LRS"
|netweaver_data_disk_size = 128
|netweaver_data_disk_caching = ""ReadWrite""
|netweaver_xscs_accelerated_networking = false
|netweaver_app_accelerated_networking = false
|netweaver_app_server_count = 2
|==========================
====== Small

HANA instance size:: Standard_E64s_v3 with xx vCPU and yy GB memory
Accelerated networking:: true

.HANA disk configuration details
----
  disks_type       = "Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS"
  disks_size       = "512,512,512,512,64,1024"
  caching          = "ReadOnly,ReadOnly,ReadOnly,ReadOnly,ReadOnly,None"
  writeaccelerator = "false,false,false,false,false,false"
  luns             = "0,1,2#3#4#5"
  names            = "datalog#shared#usrsap#backup"
  lv_sizes         = "70,100#100#100#100"
  paths            = "/hana/data,/hana/log#/hana/shared#/usr/sap#/hana/backup"
----

.Netweaver configuration details
|==========================
|netweaver_xscs_vm_size = "Standard_D2s_v3"
|netweaver_app_vm_size = "Standard_D2s_v3"
|netweaver_data_disk_type = "Premium_LRS"
|netweaver_data_disk_size = 128
|netweaver_data_disk_caching = ""ReadWrite""
|netweaver_xscs_accelerated_networking = false
|netweaver_app_accelerated_networking = false
|netweaver_app_server_count = 2
|==========================

====== Medium

HANA instance size:: Standard_M64s with xx vCPU and yy GB memory
Accelerated networking:: true

.HANA disk configuration details
----
  disks_type       = "Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS"
  disks_size       = "512,512,512,512,512,512,1024,64,1024,1024"
  caching          = "ReadOnly,ReadOnly,ReadOnly,ReadOnly,None,None,ReadOnly,ReadOnly,ReadOnly,ReadOnly"
  writeaccelerator = "false,false,false,false,false,false,false,false,false,false"
  luns             = "0,1,2,3#4,5#6#7#8,9"
  names            = "data#log#shared#usrsap#backup"
  lv_sizes         = "100#100#100#100#100"
  paths            = "/hana/data#/hana/log#/hana/shared#/usr/sap#/hana/backup"
----

.Netweaver configuration details
|==========================
|netweaver_xscs_vm_size = "Standard_D2s_v3"
|netweaver_app_vm_size = "Standard_E64s_v3"
|netweaver_data_disk_type = "Premium_LRS"
|netweaver_data_disk_size = 128
|netweaver_data_disk_caching = "ReadWrite"
|netweaver_xscs_accelerated_networking = false
|netweaver_app_accelerated_networking = true
|netweaver_app_server_count = 5
|==========================

====== Large

HANA instance size:: Standard_M128s with xx vCPU and yy GB memory
Accelerated networking:: true

.HANA disk configuration details
----
  disks_type       = "Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS"
  disks_size       = "1024,1024,1024,512,512,1024,64,2048,2048"
  caching          = "ReadOnly,ReadOnly,ReadOnly,None,None,ReadOnly,ReadOnly,ReadOnly,ReadOnly"
  writeaccelerator = "false,false,false,true,true,false,false,false,false"
  luns             = "0,1,2#3,4#5#6#7,8"
  names            = "data#log#shared#usrsap#backup"
  lv_sizes         = "100#100#100#100#100"
  paths            = "/hana/data#/hana/log#/hana/shared#/usr/sap#/hana/backup"
----

.Netweaver configuration details
|==========================
|netweaver_xscs_vm_size = "Standard_D2s_v3"
|netweaver_app_vm_size = "Standard_E64s_v3"
|netweaver_data_disk_type = "Premium_LRS"
|netweaver_data_disk_size = 128
|netweaver_data_disk_caching = "ReadWrite"
|netweaver_xscs_accelerated_networking = false
|netweaver_app_accelerated_networking = true
|netweaver_app_server_count = 10
|==========================

endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
GCP
endif::[]

ifeval::[ "{cloud}" == "Libvirt" ]
Libvirt
endif::[]

=== Salt Building Blocks

We have seen that resources are the most important elements in terraform, and there is an other resource type used as last step from the terraform process, the _Provisioner_ resource.

It can be used to model specific actions on a remote machine in order to prepare them for other services.

The terraform _file provisioner_ is used to copy directories _MAIN_/salt and _MAIN_/pillar from the machine executing Terraform to the newly created nodes.

As last step the terraform _remote-exec provisioner_ is used, to call the script _provision.sh_ on the remote node to run the salt provisioning steps. It comes from the terraform module _MAIN/generic_modules/salt_provisioner/main.tf_.

*So from this point on all work is done on the respective node itself.*

==== Our Architecture for the salt building blocks

//fixme - image our salt module arch.
//image::

Formulas: group of states give a context for building blocks e.g HANA
States: combination of execution modules and other parts, have logic in and execute to a desired state
Execution modules: basic execution modules, to provide the methods in the lower layer (shaptools) to salt
shaptools: low level python wrapper (api) around SAP utilities and commands


The provisioning workflow of the SAP building blocks consist of different steps:

1. Bootstrap salt installation and configuration
2. Execute OS setup operations. Register to SCC if needed, updated the packages etc, with help of executing the states within _/srv/salt/os_setup_
3. Execute predeployment operations with help of execution of the _/srv/salt/top.sls_ states. It update hosts and hostnames, install the formular packages, etc
4. Execute deployment operations depending on the overall configuration settings like install SAP applications and configure and setup HA with the salt formulas.


==== Salt Overview
The SAP building block are created with help of SALT formulars after provisioning the virtual machines with terraform. The formulars are shipped as RPM packages with {sles4sap}

The salt formulas can be used with 2 different approaches: salt master/minion or only salt minion execution.

With the automation solution we use the salt minion option, the steps must be executed in all of the minions where the formulas are going to executed, which is done through a ssh connection.

The core of the Salt State system is the SLS, or **S**a**L**t **S**tate file. The SLS is a representation of the state in which a system should be in, and is set up to contain this data in a simple format.

There are 3 types of salt files used
pillar files:: the _configuration_ parameters where the data gets imported with help of jinja (map.jinja) and salt['pillar.get']
state files:: the _execution_ definition in /srv/salt
grains files:: _environment_ parameters from the node itself and for handing over variables from terraform e.g. /etc/salt/grains

In Salt, the file which contains a mapping between groups of machines on a network and the configuration roles that should be applied to them is called a top file.

Top files are named _top.sls_ by default and they are so-named because they always exist in the "top" of a directory hierarchy that contains state files and this directory hierarchy is called a state tree.

===== Salt pillar

Similar to the state tree, the pillar is comprised of .sls files and has a top file too. The default location /srv/pillar.

The pillar files define custom variables and data for a system.

When Salt pillar data is refreshed, each Salt minion is matched against the targets listed in the _top.sls_ file. When a Salt minion matches a target, it receives all of the Salt pillar SLS files defined in the list underneath that target.

.Directory structure for pillars
[subs="attributes,quotes"]
----
/srv
├── pillar
│   ├── *top.sls*
│   ├── drbd
│   │   ├── cluster.sls
│   │   └── drbd.sls
│   ├── hana
│   │   ├── cluster.sls
│   │   └── hana.sls
│   ├── iscsi_srv.sls
│   └── netweaver
│       ├── cluster.sls
│       └── netweaver.sls
├── salt
...
----

The _top.sls_ pillar file describes the needed pillar data for the respective role of the node.

.State top.sls file
[subs="attributes,quotes"]
----
base:
  'role:iscsi_srv':
    - match: grain
    - iscsi_srv

  'role:hana_node':
    - match: grain
    - hana.hana

  'G@role:hana_node and G@ha_enabled:true':
    - match: compound
    - hana.cluster

  'role:drbd_node':
    - match: grain
    - drbd.drbd
    - drbd.cluster

  'role:netweaver_node':
    - match: grain
    - netweaver.netweaver

  'G@role:netweaver_node and G@ha_enabled:true and P@hostname:.*(01|02)':
    - match: compound
    - netweaver.cluster
----

To run an initial deployment without specific customization, you can use pillar files stored in the _MAIN/pillar_example/automatic` folder, as these files are customized with parameters coming from terraform execution. The pillar files stored there are able to deploy a basic functional set of clusters in all of the available cloud providers.

To adapt the deployment to your scenario, you should provide your own pillar data files
and there are some basic examples within the directory _MAIN/pillar_example_.
As the pillar files provide data for the salt-formulas, you can find all of the pillar possible options in each formula project.
// fixme
//- this need to be in a document instead of the all the different github projects
//- https://github.com/SUSE/saphanabootstrap-formula (HANA configuration)
//- https://github.com/SUSE/habootstrap-formula (HA cluster configuration)
//- https://github.com/SUSE/drbd-formula (DRBD configuration)
//- https://github.com/SUSE/sapnwbootstrap-formula (NETWEAVER or S4/HANA configuration)

[IMPORTANT]
====
Pillar files are expected to contain private data such as passwords required for the automated installation or other operations. Therefore, such pillar data need to be stored in an encrypted state, which can be decrypted during pillar compilation.

SaltStack GPG renderer provides a secure encryption/decryption of pillar data. The configuration of GPG keys and procedure for pillar encryption are described in the Saltstack documentation guide:

. https://docs.saltstack.com/en/latest/topics/pillar/#pillar-encryption[SaltStack pillar encryption]

. https://docs.saltstack.com/en/latest/ref/renderers/all/salt.renderers.gpg.html[SaltStack GPG RENDERERS]

*This is not done by the project and you need take care of this by yourself*
====


===== Salt States
_Salt state_ files are organized into a directory tree, called the Salt state tree, in the /srv/salt/ directory.

.Directory structure for Salt state files
[subs="attributes,quotes"]
----
/srv
├── pillar
....
├── salt
│   ├── cluster_node
│   │   ├──
│   ├── default
│   │   ├──
│   ├── drbd_node
│   │   ├──
│   ├── hana_node
│   │   ├──
│   ├── iscsi_srv
│   │   ├──
│   ├── _modules
│   │   ├──
│   ├── monitoring_srv
│   │   ├──
│   ├── netweaver_node
│   │   ├──
│   ├── os_setup
│   │   ├──
│   ├── provision.sh
│   ├── qa_mode
│   │   ├──
│   ├── sshkeys
│   │   ├──
│   ├── _states
│   │   ├──
│   └── **top.sls**
----

You will see within this directory structure all needed steps depending on the _role_ of the node.

The _top.sls_ file describes two environments for the nodes, _pre-deployment_ and _base_ which reflect the steps 3 and 4 of the workflow above. For each role of the nodes there more detailed files responsible.

The pre-deployment is needed, as we can not install formulas and use them directly in the same execution.

.State top.sls file
[subs="attributes,quotes"]
----
predeployment:
  'role:hana_node':
    - match: grain
    - default
    - cluster_node
    - hana_node

  'role:netweaver_node':
    - match: grain
    - default
    - cluster_node
    - netweaver_node

  'role:drbd_node':
    - match: grain
    - default
    - cluster_node
    - drbd_node

  'role:iscsi_srv':
    - match: grain
    - iscsi_srv

  'role:monitoring_srv':
    - match: grain
    - default
    - monitoring_srv

base:
  'role:hana_node':
    - match: grain
    - hana

  'G@role:hana_node and G@ha_enabled:true':
    - match: compound
    - cluster

  'role:drbd_node':
    - match: grain
    - drbd
    - cluster

  'role:netweaver_node':
    - match: grain
    - netweaver

  'G@role:netweaver_node and G@ha_enabled:true and P@hostname:.*(01|02)':
    - match: compound
    - cluster
----

===== Salt grains

SaltStack comes with an interface to derive information about the underlying system. This is called the _grains_ interface, because it presents salt with grains of information.
It collects static informations about the underlying managed system, like the operating system, domain name, IP address, kernel, OS type, memory, and many other system properties.

We use custom grains to match the roles and the further states.

The _role_ is a _custom grains_ define with help of the terraform file _salt_provisioner.tf_ for the respective building block.

CAUTION:
----
If you use the salt formulas independent from the terraform templates, you need to take care of providing all needed variables by yourself which normally get set by the _salt_provisioner.tf_.
----

===== State details

If you target a directory during a _state.apply_ or in the state Top file, salt looks for an init.sls file in that directory and applies it.

Within the _os_setup_ there is e.g. the minion configuration file

[subs="attributes,quotes"]
----
│   ├── os_setup
│   │   ├── init.sls
│   │   ├── ip_workaround.sls
│   │   ├── *minion_configuration.sls*
│   │   ├── packages.sls
│   │   ├── registration.sls
│   │   └── repos.sls
----

One interesting file is the _minion_configuration.sls_ as it provides the configuration how and where salt / the minion looks for salt states and salt formulas.

If we look deeper into one of the directories, e.g. _hana-node_ we will find more files in these directories.

.HANA Node state files
[subs="attributes,quotes"]
----
│   ├── *hana_node*
│   │   ├── download_hana_inst.sls
│   │   ├── files
│   │   │   └── sshkeys
│   │   │       ├── cluster.id_rsa
│   │   │       └── cluster.id_rsa.pub
│   │   ├── hana_inst_media.sls
│   │   ├── hana_packages.sls
│   │   ├── *init.sls*
│   │   └── mount
│   │       ├── azure.sls
│   │       ├── gcp.sls
│   │       ├── *init.sls*
│   │       ├── mount.sls
│   │       ├── mount_uuid.sls
│   │       └── packages.sls
----

Salt executes what is in _init.sls_ in the order listed in the file. When an salt file is named init.sls it inherits the name of the directory path that contains it. This formula/state can then be referenced with the name of the directory.

In our case here, it first it gets the SAP HANA Media with help of _hana_ins_media_, create the mountpoints and partition disks for SAP HANA and enter them into the fstab with help of the states in the _mount_ directory. Similar as before, the starting point is again the _init.sls_ file.

After all is processed within _mount_, it gets back to the file _hana_packages_, which then install the RPM packages _shaptools_ and _saphanabootstrap-formula_ which get shipped with {sles4sap}.

All other states files get processed in the same way as the example above.

==== Salt formula packages

Formulas are pre-written Salt States. They are as open-ended as Salt States themselves and can be used for tasks such as installing a package, configuring, and starting a service, setting up users or permissions, and many other common tasks. Each Formula is intended to be immediately usable with sane defaults without any additional configuration.

Our formulas are configurable by including data in Pillar, what we discussed above.
During RPM install, the files of the packages end up in the directory _/usr/share/salt-formulas/states_, which we had defined as directory where salt searches for file in addition to /srv/salt (see os_setup state above).

.shaptools package
If you have wondered above about the directories __modules_ and __states_, they come from the install of the package shaptools and provide a python wrapper for sap command line tools as API, in order to make it simple to be used from salt. This package is a base dependency for most of our formula packages as it provides the SAP commands.

[subs="attributes,quotes"]
----
│   ├── _modules
│   │   ├── ...
│   ├── _states
│   │   ├── ...
----

===== HANA formula

The main work of preparing the node for HANA and installing HANA is done by the _saphanabootstrap-formula_.

The structure is similar what you have seen above for pillars and states but lives in the directory _/usr/share/salt-formulas/states/..._

[subs="attributes,quotes"]
----
states/
└── hana
    ├── defaults.yaml
    ├── enable_cost_optimized.sls
    ├── enable_primary.sls
    ├── enable_secondary.sls
    ├── exporter.sls
    ├── *init.sls*
    ├── install.sls
    ├── map.jinja
    ├── packages.sls
    ├── pre_validation.sls
    └── templates
        ├── hanadb_exporter.j2
        ├── scale_up_resources.j2
        └── srTakeover_hook.j2
----

Salt includes the Jinja2 template engine which can be used in Salt state files, Salt pillar files, and other files managed by Salt.

Salt lets you use Jinja to access minion configuration values, grains and Salt pillar data, and call Salt execution modules. One of the most common uses of Jinja is to insert conditional statements into Salt pillar files.

1. The formula package is installed through the HANA Node state files
2. If you want to install it manual, please use zypper, as it will include the other dependent packages like salt-shaptools and habootstrap-formula
----
 zypper install saphanabootstrap-formula
----

The salt formula need some input data through a pillar file, which is coming from the main project file (MAIN/pillar/... or on the node /srv/pillar ), or if you use it standalone it need to be provided by you.

.Example HANA pillar
[subs="attributes,quotes"]
----
hana:
  saptune_solution: 'HANA'
  nodes:
    - host: '_hana01_'
      sid: '_prd_'
      instance: "_00_"
      password: '_SET YOUR PASSWORD_'
      install:
        software_path: '/sapmedia/HANA'
        root_user: 'root'
        root_password: ''
        system_user_password: '_SET YOUR PASSWORD_'
        sapadm_password: '_SET YOUR PASSWORD_'
      primary:
        name: _PRIMARY_SITE_NAME_
        backup:
          key_name: 'backupkey'
          database: 'SYSTEMDB'
          file: 'backup'
        userkey:
          key_name: 'backupkey'
          environment: '_hana01_:30013'
          user_name: 'SYSTEM'
          user_password: '_SET YOUR PASSWORD_'
          database: 'SYSTEMDB'

    - host: '_hana02_'
      sid: '_prd_'
      instance: "_00_"
      password: '_SET YOUR PASSWORD_'
      install:
        software_path: '/sapmedia/HANA'
        root_user: 'root'
        root_password: ''
        system_user_password: '_SET YOUR PASSWORD_'
        sapadm_password: '_SET YOUR PASSWORD_'
      secondary:
        name: _SECONDARY_SITE_NAME_
        remote_host: '_hana01_'
        remote_instance: "_00_"
        replication_mode: 'sync'
        operation_mode: 'logreplay'
        primary_timeout: 3000
----

1. The formular is executed within the salt of _hana_node_ state files
2. If you want to execute the formular manually, salt
----
salt '*' state.apply hana_node.sls
----
//fixme - check if this is ok.

So with help of the pillar data and the state file and the formular, salt will create all needed configuration on the node, installs HANA and if enabled install hana systemreplication and set up the pacemaker cluster, right for {cloud}.

The _templates_ directory provides the needed files for cluster rules, the needed hook for HANA and the monitoring exporter.  All the values come from the best practice guides SUSE created with the Cloudprovider {cloud} for the HA scenario.

===== Netweaver formula

The SAP Netweaver deployment is performed using the _sapnwbootstrap-formula_ and uses as of today only SAP HANA as a database.

The formula takes care of the ASCS, the Application Servers and if HA is selected of a Enqueue Replication server.

The formula has some hard dependencies and all of them must be in place for a successful netweaver deployment. In order to deploy a correct Netweaver environment a NFS share is needed (SAP stores some shared files there). The NFS share must have the folders _sapmnt_ and _usrsapsys_ in the exposed folder.
The folders are created with the Netweaver SID name (for example /sapdata/HA1/sapmnt and /sapdata/HA1/usrsapsys). This subfolders content is removed by default during the deployment.

Second, the SAP installation software (swpm) must be available in the system. To install the whole Netweaver environment with all the 4 components, the swpm folder, sapexe folder, Netweaver Export folder and HANA HDB Client folders must already exist, or be previously mounted when provided by external service, like NFS share. The netweaver.sls pillar file must be updated with all this information. Netweaver Export and HANA HDB Client folders must go in additional_dvds list.

The structure is similar what you have seen above for the HANA formula.

[subs="attributes,quotes"]
----
states/
└── ...
└── netweaver
    ├── defaults.yaml
    ├── ensa_version_detection.sls
    ├── extract_nw_archives.sls
    ├── ha_cluster.sls
    ├── *init.sls*
    ├── install_aas.sls
    ├── install_ascs.sls
    ├── install_db.sls
    ├── install_ers.sls
    ├── install_pas.sls
    ├── install_pydbapi.sls
    ├── map.jinja
    ├── monitoring.sls
    ├── pillar.example
    ├── pre_validation.sls
    ├── saptune.sls
    ├── setup
    │   ├── init.sls
    │   ├── keepalive.sls
    │   ├── mount.sls
    │   ├── packages.sls
    │   ├── sap_nfs.sls
    │   ├── shared_disk.sls
    │   ├── swap_space.sls
    │   ├── users.sls
    │   └── virtual_addresses.sls
    └── templates
        ├── aas.inifile.params.j2
        ├── ascs.inifile.params.j2
        ├── cluster_resources.j2
        ├── db.inifile.params.j2
        ├── ers.inifile.params.j2
        └── pas.inifile.params.j2
----

As you know from earlier descriptions, we need a pillar file with the configuration. There is one example in the path which could be used as base for a standalone salt usage. In general the pillar data get handed over from the terraform main project.

As SAP Netweaver has in an HA environment more nodes, therefore the pillar file is much bigger as eg. the one for HANA. So please have a look by yourself of the example file.

Similar as before, the starting point is the _init.sls_ file where the workflow is defined.

The _templates_ directory provides the needed files for NW cluster rules and the values come from the best practice guides SUSE created with the Cloudprovider {cloud} for the ERS scenario.

In addition here are the templates which are used by SWPM for a automated handfree installation of the SAP Netweaver services.

==== High Availability formula

The _habootstrap-formula_ provide the needed cluster setups for SAP HANA, SAP Netweaver, or if needed for the HA NFS service build with drbd.
It will take care of

The formula will be, similar to all the other formulas used, installed in /usr/share/salt-formulas/states/cluster.

[subs="attributes,quotes"]
----
states
├── cluster
│   ├── create.sls
│   ├── defaults.yaml
│   ├── *init.sls*
│   ├── join.sls
│   ├── map.jinja
│   ├── monitoring.sls
│   ├── ntp.sls
│   ├── packages.sls
│   ├── pre_validation.sls
│   ├── remove.sls
│   ├── resource_agents.sls
│   ├── sshkeys.sls
│   ├── support
│   │   └── ssh_askpass
│   └── watchdog.sl
----

The main difference to the HANA and Netweaver formula is that the _init.sls_ make already use of _jinja_.
Jinja is the default templating language in SLS files and get evaluated before YAML, which means it is evaluated before the States are run.

The most basic usage of Jinja in state files is using control structures to wrap conditional or redundant state elements


==== Additional Services

The additinal services depend on what is used or available of the cloudprovider, but needed by SAP HANA or SAP Netweaver or the HA services.

===== NFS service

To build an HA-NFS service, we use the above describe _habootstrap-formula_ together with _drbd-formula_ to mirror the data between two nodes and the _linux nfs-server: packages been setup with the saltstack _nfs_formula ( see https://github.com/saltstack-formulas/nfs-formula )

DRBD®– software is a distributed replicated storage system for the Linux platform. It is implemented as a kernel driver, several userspace management applications, and some shell scripts. So simplified, think about it as an raid-1 over network.

Details are available at the SUSE documentation page for the SLE HA Extension
https://documentation.suse.com/sle-ha/15-SP2/single-html/SLE-HA-nfs-quick/#art-sleha-nfs-quick


===== Fencing service

If the setup is using HA for SAP Netweaver or SAP HANA or with the NFS service, and there is mechanism for fencing of the virtual machines over an API we use the SUSE method of using a SBD-device. Such a SBD-Device is normally a raw shared disk beween two nodes.

Unfortunately not all clouds are able to provide a raw shared disk, but with the help of linux native services (iSCSI) we can build this by our own.

We use here the _iscsi-formula_ provided by saltstack itself, see https://github.com/saltstack-formulas/iscsi-formula to provide the nodes of the cluster a raw-shared-disk with help of a _iscsi target_ for the SBD fencing mechanism.

It gets configured through the pillar files we provided through the role _iscsi_srv_

The use of possible fenching method depends on the cloud providers possibilities. As of today SBD is needed only for Azure, but it is a general method which could be used nearly independent of the base infrastructure.

// fixme - add monitoring
//===== Monitoring service
//golang-github-prometheus-node_exporter
//prometheus-ha_cluster_exporter
//prometheus-hanadb_exporter
//prometheus-sap_host_exporter


//ifeval::[ "{cloud}" == "Azure" ]
//Azure
//endif::[]
//
//ifeval::[ "{cloud}" == "AWS" ]
//AWS
//endif::[]
//
//ifeval::[ "{cloud}" == "GCP" ]
//GCP
//endif::[]
//
//ifeval::[ "{cloud}" == "Libvirt" ]
//Libvirt
//endif::[]
//
//
//image::SA-Physical.png[title="Solution Architecture - {useCase} Physical", scaledwidth=80%]
//
//ifdef::Availability[]
//include::./Availability/SA.adoc[]
//endif::Availability[]
//
//ifdef::Performance[]
//include::./Performance/SA.adoc[]
//endif::Performance[]
//
//ifdef::Security[]
//include::./Security/SA.adoc[]
//endif::Security[]
//
//ifdef::Integrity[]
//include::./Integrity/SA.adoc[]
//endif::Integrity[]

