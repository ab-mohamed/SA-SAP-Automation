
== Technology

////
The Technology Layer elements are typically used to model the Technology Architecture of the enterprise, describing the structure and behavior of the technology infrastructure of the enterprise.

* *_How_* various technology components can facilitate this

Technology components utilized as a part of this solution: CSP Specific, Networking, Instance Types, etc.

## Technology (attributes)

#ADOC_ATTRIBUTES+=" --attribute Azure=1"
#ADOC_ATTRIBUTES+=" --attribute instances-Azure=1"
#ADOC_ATTRIBUTES+=" --attribute SBD-Storage-Azure=1"

////

=== Terraform

With help of terraform and the modules for the cloudprovider we create the infrastructure for the SAP application and some supporting services.

The Terraform templates provide everything needed to create the right infrastructure components and provide in addition a lot of predefined possible choices to make it more simple to create the right virtual machines, disks, networks etc.

=== Salt

What is Salt?

Salt is a different approach to infrastructure management, founded on the idea that high-speed communication with large numbers of systems can open up new capabilities. This approach makes Salt a powerful multitasking system that can solve many specific problems in an infrastructure.

The backbone of Salt is the remote execution engine, which creates a high-speed, secure and bi-directional communication net for groups of systems. On top of this communication system, Salt provides an extremely fast, flexible, and easy-to-use configuration management system called Salt States.

Our SAP landscape is made up of groups of machines, each machine in the group performing a role. Those groups of machines work in concert with each other to create an application stack.

To effectively manage those groups of machines, an administrator needs to be able to create roles for those groups. As example, a group of machines that serve front-end web traffic might have roles which indicate that those machines should all have the webserver package installed and that the web service should always be running.

In Salt, the file which contains a mapping between groups of machines on a network and the configuration roles that should be applied to them is called a _top file_.

Top files are named _top.sls_ by default and they are so-named because they always exist in the "top" of a directory hierarchy that contains state files. That directory hierarchy is called a state tree and this is what we use to reference the building blocks for the SAP Landscape.

==== SAP Sizing

To make the SAP sizing more simple we have introduced pre-defined sizes with well known abbreviation from T-Shirt sizes Small (S), Medium (M), Large (L).

The Small (S) size is thought for non productive scenarios, where as the Medium (M) and Large (L) sizes for production setups, where certified instance types being used.

But you can also tweak such size or do complete your own settings.

As sizing is important, you need to choose the right instance sizes from the cloud provider which are certified by SAP and choose the right number of disks to provide the right I/O, as similar the network throughput for the solution.

==== Building Blocks

Our main building blocks for an SAP Landscape are the _Application Layer_, based on Netweaver with SAP Central Services (xSCS) with an Primary Application Server (PAS) and Additional application server (AAS) and the _Database Layer_, which is in our case SAP HANA.

There are two possible models how you can deploy SAP Business Suite, a centralized deployment where all runs on one server or distributed deployment where every service has its own node.

The centralized deployment is mostly used for non-production environments such as sandbox and development environments.

//todo - picture

The distributed deployment, is the recommend way for production environments where each of the SAP application layer components is independently installed on different instances.
Which is the scenario we have choosen for the automation.

//todo - picture

We use the needed building blocks to build this landscape within the top.sls file and depending on the role of the node, the needed actions are triggered.

==== High Availability

The two main building blocks could be made high available in order to have less downtime and eliminate single point of failures. As there are many scenarios available from SAP, we support the most used scenarios, with the solution.

For the Central Services this is Enqueue Replication and for the database its the HANA Systemreplication.

This mean we need to create one additional machine for the building blocks to build a two-node cluster within the HA Scenarios.

ifeval::[ "{cloud}" == "Azure" ]

To provide something like a "virtual IP address" which is able to move between the two cluster nodes, we use the _Standard Load Balancer_ service from Azure to provide traffic
to only the active node.

image::azure_loadbalancer.png[scaledwidth="80%"]

endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
GCP
endif::[]

ifeval::[ "{cloud}" == "Libvirt" ]
Libvirt
endif::[]


==== Additional Services

Depending on offered services from the cloud provider, we may need to build some services by our own if they are not there or use whats available, and similar to the above steps its reflected within the top.sls file.

===== NFS service

ifeval::[ "{cloud}" == "Azure" ]

As we started with Azure, there was no NFS service available, so we need to build some with the tools we ship in {sles4sap}. As the NFS service should be high available, we need a second virtual machine to build a two node cluster.

image::Azure_HA_NFS_Service.png[scaledwidth="80%"]

Over the time, Azure provide more and more services. So as of time of writing, there is a native NFS service with help of Netapp available (Azure Netapp files - ANF) and the Azure file service is extending in this direction too.
//todo - link

endif::[]

ifeval::[ "{cloud}" == "AWS" ]
On AWS, Shared SAP resources are managed in AWS Elastic File Systems (EFS). This provides the NFS services required to support the SAP deployment.
 
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
GCP
endif::[]

ifeval::[ "{cloud}" == "Libvirt" ]
Libvirt
endif::[]

If the cloud native NFS service is used, not additional virtual machines will be created and the native service need to be set up in forehand.

===== Fencing service

For high available clusters, we need a mechanism to switch off one machine in the case of a so called "split-brain", mean when they can not communicate with each other anymore.

There are several methods which can be used and it depends again of the possibilities of the infrastructure.

ifeval::[ "{cloud}" == "Azure" ]
As we started with Azure, Microsoft and SUSE created a fencing agent for the cluster. Such a fencing agent should be remove a machine as fast as possible (immediate) from the cluster, to make sure that there is only one active node, in order to avoid data corruption.

At this point of time, the Azure infrastructure provided only a way to graceful shutdown a machine, which took 10-15 minutes - which is by far too long for the split-brain case.

We need to create our own mechanism to fence machines. One technology we provide within our HA Extension is, using storage as additional communication between the nodes for such a split-brain case. This needs a raw shared disk, in order that both nodes can write messages to a central place. It is called SBD - Stonith Block Device or Split Brain Detector.

Unfortunately the Azure infrastructure did, at this point, not provide such a raw disk service, so we need to build it with the Linux tools we have in the distribution. With help of a iSCSI server, we can provide a raw shared disk within the cloud and therefore we are able to use the SBD fencing method which with help of the linux watchdog mechanism provides a fast and reliable fencing tooling.

This mean one additional server to provide a iSCSI service.

image::Azure_fence_iscsi.png[scaledwidth="80%"]

// fixme - this is not implemented from the automation as of today
In the meantime there is a way in the Azure API to "kill" a virtual machine, so that the fencing agent can make use of it and no additional machine is needed if the fence agent is used.
The drawback here is, in order to talk to the Azure API, there is a need for public network connection.

image::Azure_fence_arm.png[scaledwidth="80%"]

So you can choose between two ways
. SDB fencing with help of an iSCSI service
. agent based fencing through API access
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS Supports the use of the AWS EC2 STONITH mechanism.  This is shipped and supported with the SUSE HA Extension and has been specifically written to fence (poweroff/reboot etc) EC2 instances as part of cluster operations.

Behind the scenes, it uses the AWS CLI, EC2 Tags and IAM as a method to securely identify a node and then fence it. 

// Fixme - fact check. It requires internet connectivity to ensure the EC2 endpoint can be reached.

A working STONITH method is mandatory to run a supported SUSE cluster on AWS.

endif::[]

ifeval::[ "{cloud}" == "GCP" ]
GCP
endif::[]

ifeval::[ "{cloud}" == "Libvirt" ]
Libvirt
endif::[]

===== Monitoring service

If you decide to use also the monitoring we provide with help of Prometheus and Grafana, we need a additional virtual machine for this service.
